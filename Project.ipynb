{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring classification algorithms applied on the iris flower data set.\n",
    "\n",
    "2023/24\n",
    "\n",
    "By Trish O'Grady\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## The Iris Dataset\n",
    "\n",
    "In 1936, Ronald Fisher, a statistician and biologist developed a linear function to differentiate Iris plant species based on the morphology of their flowers. The Fisher’s Iris data set contains 50 samples of the 3 Iris species. They are Iris Setosa, Iris Virginica and Iris versicolor. The dataset contains 4 features - the widths and lengths of petals and sepals. Its reguraly used to test machine learning alogoritms.[0]\n",
    "\n",
    "## What is Supervised Learning?\n",
    "\n",
    "Supervised learning is a type of machine Learning that uses labeled training data to teach an algorithm how to make predictions on its own. Because it comprises a training phase where the algorithm is given a dataset that contains both input data and the appropriate output or target values, it is known as a \"supervised\" learning method.[1] The algorithm then learns by generalizing from the training data how to translate the input data to the desired output.\n",
    "There are two types of supervised learning. One type is classification whereby the objective of classification issues is to assign input data to one of a number of predefined classes or labels.[2] For example each of the three flower classes in the iris dataset—Versicolor, Setosa, and Virginica—has four features: sepal length, sepal width, petal length, and petal width. The classification of iris flowers aims to predict flowers based on their distinctive characteristics. The second type is Regression whereby regression problems are used to predict a numerical value or quantity based on the input data, which can be utilized for tasks like predicting the value of a house based on variables like location, semi-detached, private garden and so forth.[2]\n",
    "\n",
    "The key components to supervised learning are as follows:\n",
    "Input Data or Features. These are the parameters or variables that define the input. In a classification problem, these could be for example the measurements of an object. These might be variables influencing a numerical result in a regression problem.[3]\n",
    "Output data or Labels. The output data from supervised learning represents the predictions that the algorithm is attempting to make. Labeled data refers to training data that has already been associated with the desired result. For instance, the output in a classification problem might be a category label, while in a regression problem might be a number.[3]\n",
    "Training Data: The input-output pairs make up the training dataset. It is applied to the machine learning model's training. This information is used by the algorithm to discover patterns and connections between the features of the input and the related output.[3]\n",
    "Model Learning: The machine learning model learns the fundamental patterns, relationships, and rules that map input data to output data using the training data. The objective is to develop a model that generalizes well and makes precise assumptions about new data.[3]\n",
    "Prediction: The model takes the input data, analyses it, and generates a prediction or conclusion as an output. Once the model has been trained, it may be used to make predictions on new data.[3]\n",
    "\n",
    "Overall, supervised learning is a fundamental and effective machine learning approach that serves as the foundation for numerous practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are classification algorithms?\n",
    "\n",
    "Machine learning applications use classification algorithms, a subset of supervised learning algorithms, to divide data into discrete classes or categories based on input features. These algorithms are made to recognize and learn from data patterns, which may then be used to categorize data points into predetermined groups.[4]\n",
    "Logistic regression is employed for binary classification issues when there are only two possible classifications for the output. The logistic function is used to model the likelihood that an input belongs to a given class.[4]\n",
    "In order to make judgments, decision trees iteratively divide the data into subsets according to the most important attribute. Both binary and multi-class classification can be done using them.[5]\n",
    "Multiple decision trees are combined in Random Forest, an ensemble learning technique, to increase accuracy and decrease overfitting. It works well for tasks involving classification and regression.[5]\n",
    "The kind of problem, the type of data, and the specific criteria all influence the choice of classification algorithm. It's normal practice to test out various algorithms to see which one does the task most effectively.[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An overview of the scikit-learn Python library\n",
    "\n",
    "Scikit-Learn is a free machine learning library for Python. It provides tools for data analysis and modelling. It offers a variety of methods for classification, regression, clustering, and dimensionality reduction and supports both supervised and unsupervised machine learning.[6]\n",
    "\n",
    "Giving an input a label or category based on its features or qualities is the basic function of classification in machine learning.\n",
    "Scikit-learn handles classification in a number of ways. Some algorithms include Logistic Regression, Principle Componant Analysis(PCA), Random Forests and Decision Trees. These are helpful for classification and regression issues.[7]\n",
    "\n",
    "Before choosing an algorithm for classification, data needs to be prepared and labelled so it is classified. After choosing a suitable algorithm like Logistic Regression, the model needs to be trained so it can find patterns within the dataset. Metrics then need to be applied to evaluate the performance. Once the model is trained it can be used to make predictions.[7]\n",
    "\n",
    "Like classification, regression is a supervised machine learning task that aims to predict a continuous numeric output variable using input information. It is used for estimating values, or modelling relationships between variables. It too needs prepared data before choosing an algorithm. The model is also trained and metrics are applied before making predictions.[6]\n",
    "\n",
    "In conclusion, scikit-learn is a flexible and popular Python framework for data analysis and machine learning. It is a crucial component of the toolset for data scientists and machine learning practitioners since it offers a complete set of tools for developing, assessing, and deploying machine learning models.[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\trish\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\trish\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\trish\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\trish\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\trish\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\trish\\anaconda3\\lib\\site-packages (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import datasets\n",
    "\n",
    "#iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_path = r\"C:\\Users\\Trish\\repo\\MachineLearningandStatistics\\Machine_Learning_And_Statistics\\iris_dataset.csv\"\n",
    "csv_path = r\"C:\\Users\\kehin\\Downloads\\iris_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\kehin\\\\Downloads\\\\iris_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##Create a dataframe from the csv dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\kehin\\\\Downloads\\\\iris_dataset.csv'"
     ]
    }
   ],
   "source": [
    "##Create a dataframe from the csv dataset\n",
    "df=pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preview the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check for null values in the dataframe\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check the description of the data in the Dataframe.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning \n",
    "\n",
    "Machine Learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence (AI) based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.[8]\n",
    "\n",
    "Machine Learning involves 'training' the model, which includes using a subset of a dataset, in which the performance of the model is unknown until it is 'tested' on additional data that was not available during training, referred to as the test set. In this situation, the goal of machine learning is to achieve the best results on the test set.[8]\n",
    "The goal of (supervised) machine learning is to create a model that can generate accurate predictions over time. Machine learning is all about results, ie the output.  Statistical modeling, on the other hand, is more concerned with identifying correlations between variables and their significance, but it can also make predictions.[9]\n",
    "\n",
    "The generated data can be utilised to train the Machine Learning model of choice to make predictions in the real world after many different simulations have run and tested all of the different conceivable situations. [10]\n",
    "\n",
    "Initially, it is assumed that there is a null hypothsis, which is an initial statement in Machine Learning that claims there is no association between two measured events. However, an alternative hypothesis can be accepted meaning that the data is drawn from a distribution that is different to it. The aim of the iris flower classification is to predict flowers based on their specific features. There is a probabilty threshold that determines when you reject the null hypothsis - p values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the Distribution of the Target Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pairplot with Seaborn\n",
    "# The target variable determines the colours. Many of the pairwise pairings lead to distinct clusters.\n",
    "# The machine learning model should be able to recognize the distinctions between the groups with ease \n",
    "# because there are noticeable disparities between them.\n",
    "sns.pairplot(df, hue='target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the features and target\n",
    "\n",
    "The target variable is split from all the other variables, to predict an outcome. All the variables such as sepal length, sepal width etc are used to determine what the target will be. The target variable is removed from the dataframe and stored elsewhere. Two variables are created. All the features are stored in variable X (the target column is dropped from this variabel) and the target is stored in variable Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='target', axis=1)\n",
    "\n",
    "Y = df['target']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "Y = labelencoder.fit_transform(Y)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression is the classification algorithm that is implemented using scikit-learn Python\n",
    "\n",
    "In its most basic form, logistic regression is a statistical model that uses a logistic function to model a binary dependent variable.11]\n",
    "\n",
    "The logistic model is used in statistics to model the probability of a specific class or event, such as pass/fail, win/lose etc. This can be used to represent a variety of occurrences, such as determining whether an image contains a cat, dog, or other animal. Each detected object in the image would be assigned a probability ranging from 0 to 1, with a total of one. [12]\n",
    "\n",
    "A variable needs to be declared as a model. Then the regression model needs to be loaded into it. The machine learning model needs to be trained with training data using the model.fit function. It finds the relationship between all the variables in the data set, with the target variable used for prediction. The trained model is used to predict new outcomes.[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test data\n",
    "\n",
    "Four variables are created - X_train, X_test, Y-train, Y_test.  The X and Y variables are both split into X and Y test and train data. Test size is the % of the test data required in this case 20% of the data. Stratify=Y distributes the values 0 and 1 evenly. \n",
    "The mean values change, that is, the data is split each time the mean function is run. To prevent this, and get the same split each time, the parameter Random State is used to specify the random state instance.\n",
    "\n",
    "It returns the number of data points in each array. X.shape is the original data. Training data is 80% of the data and 20% is test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, stratify=Y, random_state = 2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In artificial intelligence, model training is the process of using a dataset to educate an algorithm or model how to identify patterns, anticipate outcomes, or carry out particular tasks. In order to reduce the discrepancy between expected and actual results, the model's parameters or weights are adjusted once the training data is fed into it.[13]\n",
    "\n",
    "Various strategies are included in model training, depending on the task, the type of data, and the methods employed. One model training technique is supervised learning which can be broken down into classification and regression techniques.\n",
    "\n",
    "For classification tasks, the Logistic Regression supervised learning method was appplied. As it returned 100% accuracy other supervised learning methods were applied to confirm that this result is desirable and correct and not in fact overfitting. \n",
    "\n",
    "The Decision Tree Classifier was applied. Based on feature values, decisions are made at each node in the structure, which resembles a flowchart. It divides the feature space into regions and uses a tree structure based on feature thresholds to predict the class label. Based on the characteristic that yields the most information gain, it separates at each node.[14] It returned 96.7%\n",
    "\n",
    "An ensemble learning technique based on decision trees is called the Random Forest Classifier. In order to increase accuracy and decrease overfitting, it builds several decision trees during training and combines their predictions.[15] It also returned 96.7\n",
    "\n",
    "Finally, Support Vector Machine (SVC) is another type of machine learning whereby every training example is a bag containing several instances (data points). The bag's label is based on whether or not there is at least one positive instance[16] It also returned 96.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistics Regression using Statsmodels Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model=sm.MNLogit(Y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistics Regression using Sklearn Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model = LogisticRegression(random_state = 0, solver='lbfgs', multi_class='auto')\n",
    "model.fit(X_train, Y_train)\n",
    "L_y_pred = model.predict(X_test)\n",
    "Train_score = model.score(X_train, Y_train)\n",
    "Test_score = model.score(X_test, Y_test)\n",
    "\n",
    "print(f'The training accuracy of the Logistic Regression is: {Train_score*100:.2f}%')\n",
    "print(f'The validation accuracy of the Logistic Regression is: {Test_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, Y_train) \n",
    "D_y_pred = model.predict(X_test) \n",
    "Train_score = model.score(X_train, Y_train)\n",
    "Test_score = model.score(X_test, Y_test)\n",
    "\n",
    "print(f'The training accuracy of the Decision Tree model is: {Train_score*100:.2f}%')\n",
    "print(f'The test accuracy of the Decision Tree model is: {Test_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "R_y_pred = random_forest.predict(X_test)\n",
    "\n",
    "Train_score = model.score(X_train, Y_train)\n",
    "Test_score = model.score(X_test, Y_test)\n",
    "\n",
    "print(f'The training accuracy of the Random Forest model is: {Train_score*100:.2f}%')\n",
    "print(f'The test accuracy of the Random Forest model is: {Test_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Support Vector Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "S_y_pred = svc.predict(X_test)\n",
    "\n",
    "Train_score = model.score(X_train, Y_train)\n",
    "Test_score = model.score(X_test, Y_test)\n",
    "\n",
    "print(f'The training accuracy of the Support vector model is: {Train_score*100:.2f}%')\n",
    "print(f'The test accuracy of the Support vector model is: {Test_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Using KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "KNN.fit(X_train, Y_train)\n",
    "K_y_pred = svc.predict(X_test)\n",
    "\n",
    "Train_score = model.score(X_train, Y_train)\n",
    "Test_score = model.score(X_test, Y_test)\n",
    "\n",
    "print(f'The training accuracy of the K-Neighbors model is: {Train_score*100:.2f}%')\n",
    "print(f'The test accuracy of the K-Neighbors model is: {Test_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "# Accuracy score of Training Data\n",
    "\n",
    "An accuracy score is used as the evaulation metric, ie the model will be trained on its performance. The target value is stored in the x_train_prediction and this will be compared with the original. The model will predict the target and the predicted value will be compared with the original target values. A variable is created  to store the value in this particular variable and the accuracy value is stored in this variable.[17]\n",
    "\n",
    "In machine learning, 100% accuracy is not always desirable or right because it could be a sign of overfitting or an error has occurred. When a model is overfitted, it acquires training set features to such an extent that it is unable to generalize to data that is not included in the validation and evaluation sets1. But 100% accuracy might also be achievable if the validation set is flawlessly representative of the test set and there are no problems with the code or data pipeline.[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the accuracy scorce of all models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Accuracy = round(accuracy_score(Y_test, L_y_pred)*100, 2) #Logistics Regression\n",
    "D_Accuracy = round(accuracy_score(Y_test, D_y_pred)*100, 2)\n",
    "R_Accuracy = round(accuracy_score(Y_test, R_y_pred)*100, 2)\n",
    "S_Accuracy = round(accuracy_score(Y_test, S_y_pred)*100, 2)\n",
    "K_Accuracy = round(accuracy_score(Y_test, K_y_pred)*100, 2)\n",
    "\n",
    "Model = [\"K-Neighbors Classifier\", \"Logistics Regression\", \"Decision Tree Classifier\", \"Random Forest Classifier\", \n",
    "         \"Support Vector Classifier\"]\n",
    "\n",
    "Model_accuracy = {'Model': Model,  'Accuracy Score': [K_Accuracy, L_Accuracy, D_Accuracy, R_Accuracy, S_Accuracy]}\n",
    "Model_pred = {'Model': Model, 'Predictions': [K_y_pred, L_y_pred, D_y_pred, R_y_pred, S_y_pred]}\n",
    "              \n",
    "df_Model_accuracy = pd.DataFrame(Model_accuracy)\n",
    "df_Model_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Model_accuracy\n",
    "\n",
    "best_model = df_Model_accuracy.loc[df_Model_accuracy['Accuracy Score'].idxmax()][0]\n",
    "best_accuracy = df_Model_accuracy.loc[df_Model_accuracy['Accuracy Score'].idxmax()][1]\n",
    "\n",
    "print(f'{best_model} is the best model with an accuracy score of {best_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = Model.index(best_model)\n",
    "best_y_pred = Model_pred['Predictions'][Model.index(best_model)]\n",
    "\n",
    "accuracy = accuracy_score(Y_test, best_y_pred, normalize = True) \n",
    "precision = precision_score(Y_test, best_y_pred, average='weighted') \n",
    "recall = recall_score(Y_test, best_y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "f1 = f1_score(Y_test, best_y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(Y_test, best_y_pred)\n",
    "# -----------------------------------\n",
    "print(f\"Accuracy: {accuracy*100:.2f}\")\n",
    "print(f\"Precision: {precision*100:.2f}\")\n",
    "print(f\"Recall: {recall*100:.2f}\")\n",
    "print(f\"F1: {f1*100:.2f}\\n\")\n",
    "print(\"Confusion Matrix \\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy score of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find accuracy on the Test Data\n",
    "\n",
    "test_data_accuracy =round(accuracy_score(Y_test, best_y_pred)*100, 2)\n",
    "\n",
    "print(f'Accuracy on test data : {test_data_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The accuracy score is {test_data_accuracy}%. \\nBoth accuracy scores for the training and test data should be similiar. \\nIf for example the training data had a larger percentage it would mean the model is overfitted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_scorce is a metric for regression not classification\n",
    "'''from sklearn.metrics import r2_score\n",
    "rsquared = r2_score(X_test_prediction, Y_test)\n",
    "print(\"Coefficient of determination(r^2): {}\".format(rsquared))'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "    0.  https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\t1. https://data-flair.training/blogs/iris-flower-classification/\n",
    "\t2. https://www.geeksforgeeks.org/regression-classification-supervised-machine-learning/\n",
    "\t3. https://www.sciencedirect.com/topics/computer-science/supervised-learning\n",
    "    4. https://scikit-learn.org/stable/auto_examples/classification/index.html\n",
    "    5. https://www.datacamp.com/blog/classification-machine-learning\n",
    "    6. https://scikit-learn.org/stable/index.html\n",
    "    7. https://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/\n",
    "    8. https://www.geeksforgeeks.org/data-science-vs-machine-learning/\n",
    "    9. https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3\n",
    "    10.https://towardsdatascience.com/modelling-and-simulations-in-data-science-b3f546a953d1\n",
    "    11. https://en.wikipedia.org/wiki/Logistic_regression\n",
    "    12. Choosing and Using Statistics: A Biologist's Guide, 3rd Edition, Dytham.C (2011)\n",
    "    13. https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets\n",
    "    14. https://www.geeksforgeeks.org/decision-tree/\n",
    "    15. https://en.wikipedia.org/wiki/Random_forest\n",
    "    16. https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "    17. https://datascience.stackexchange.com/questions/28426/train-accuracy-vs-test-accuracy-vs-confusion-matrix\n",
    "    18. https://www.iguazio.com/glossary/model-accuracy-in-ml/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
