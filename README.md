# Machine_Learning_And_Statistics 

By Trish O'Grady

## How to use this Repository

Step 1. Install Anaconda


## An overview of the scikit-learn Python library

Scikit-Learn is a free machine learning library for Python. It provides tools for data analysis and modeling. It offers a variety of methods for classification, regression, clustering, and dimensionality reduction and supports both supervised and unsupervised machine learning.

Giving an input a label or category based on its features or qualities is the basic function of classification in machine learning.
Scikit-learn handles classification in a number of ways. Some algorithms include Random Forests and Decision Trees. These are helpful for classification and regresssion issues. K-Nearest Neighbour (KNN) is a straighforward yet powerful instance based categorisation technique.

Before choosing an algorithm for classification, data needs to be prepared and labelled so it is classified. After choosing a suitable algorithm like Random Forest or Nearest Neighbour, the model needs to be trained so it can find patterns within the dataset. Metrics then need to be applied to evaluate the performance. Once the model is trained it can be used to make predictions.

Like classification, regression is a supervised machine learning task that aims to predict a continuous numeric output variable using input information. It is used for estimating values, or modelling relationships between variables. It too needs prepared data before choosing an algorithm. The model is also trained and metrics are applied before making predictions.

Unsupervised machine learning tasks like clustering entail assembling related data based on their properties. Without prior knowledge of class labels, like in supervised machine learning tasks, clustering aims to find innate patterns or structure in data.

Dimensionality reduction reduces the number of features (or dimensions) in a dataset, by preserving the most crucial data and patterns. Working with high-dimensional data can be difficult because of issues including overfitting, the dimensionality curse, and increased computing complexity. In order to solve these problems, dimensionality reduction techniques translate or project data into a lower-dimensional domain. It ultimately reduces the number of random variables to consider. A common way of doing this is using the Principle Componant Analysis (PCA). With PCA, the goal is to identify a fresh set of orthogonal axes—known as principal components—that best describe the data's overall variation. These parts are linear combinations of the original features.

Model Selection...

Preprocessing...






https://scikit-learn.org/stable/index.html